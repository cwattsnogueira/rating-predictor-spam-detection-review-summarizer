# -*- coding: utf-8 -*-
"""Fine-tuningBERTOversampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jrfpULYoz_lfZX1RCg8jx6MkC8QWfiY3
"""

!pip install transformers datasets scikit-learn joblib matplotlib seaborn --quiet

import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler
from datasets import Dataset
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
from torch.nn.functional import softmax
from tqdm import tqdm
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_parquet("/content/engineered_features_with_anomalies.parquet")
df = df[['clean_text', 'fake_review_label']].dropna()

# Oversample class 1 (fake reviews)
df_fake = df[df['fake_review_label'] == 1]
df_balanced = pd.concat([
    df,
    df_fake.sample(n=len(df[df['fake_review_label'] == 0]), replace=True)
])

sns.countplot(x='fake_review_label', data=df_balanced, palette='Set2')
plt.title("Balanced Fake Review Label Distribution")
plt.xlabel("Label (0 = Genuine, 1 = Fake)")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(df_balanced['fake_review_label']),
                                     y=df_balanced['fake_review_label'])
class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

dataset = Dataset.from_pandas(df_balanced[['clean_text', 'fake_review_label']])
dataset = dataset.train_test_split(test_size=0.2, seed=42)

def tokenize_function(example):
    return tokenizer(example['clean_text'], truncation=True, padding='max_length', max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'fake_review_label'])

train_dataloader = DataLoader(tokenized_dataset['train'], batch_size=16, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset['test'], batch_size=16)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
num_epochs = 10
lr_scheduler = get_scheduler("linear", optimizer=optimizer,
                             num_warmup_steps=0,
                             num_training_steps=num_epochs * len(train_dataloader))

loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))

best_val_loss = float('inf')
patience = 2
counter = 0

for epoch in range(num_epochs):
    model.train()
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}"):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        labels=batch['fake_review_label'])
        loss = loss_fn(outputs.logits, batch['fake_review_label'])
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step(); lr_scheduler.step(); optimizer.zero_grad()

    # Validation
    model.eval(); val_loss = 0
    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            labels=batch['fake_review_label'])
            loss = loss_fn(outputs.logits, batch['fake_review_label'])
            val_loss += loss.item()

    avg_val_loss = val_loss / len(eval_dataloader)
    print(f"Epoch {epoch} - Validation Loss: {avg_val_loss:.4f}")
    if avg_val_loss < best_val_loss:
        best_val_loss, counter = avg_val_loss, 0
        torch.save(model.state_dict(), "best_model_oversampled.pt")
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered."); break

model.load_state_dict(torch.load("best_model_oversampled.pt"))
model.eval()

all_preds, all_labels = [], []

with torch.no_grad():
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'])
        preds = torch.argmax(outputs.logits, dim=-1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch['fake_review_label'].cpu().numpy())

report = classification_report(all_labels, all_preds, zero_division=0, output_dict=True)
print(classification_report(all_labels, all_preds, zero_division=0))

cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix (Oversampling)")
plt.tight_layout()
plt.show()

for i in range(2):
    correct = np.sum((np.array(all_labels) == i) & (np.array(all_preds) == i))
    total = np.sum(np.array(all_labels) == i)
    acc = correct / total if total > 0 else 0
    print(f"Accuracy for Class {i} ({['Genuine','Fake'][i]}): {acc:.2%}")

joblib.dump({
    "classification_report": report,
    "confusion_matrix": cm.tolist(),
    "class_weights": class_weights.tolist()
}, "bert_oversampling_metrics.pkl")

torch.save(model.state_dict(), "final_oversampling_model.pt")
model.save_pretrained("bert_oversampling_model")
tokenizer.save_pretrained("bert_oversampling_model")