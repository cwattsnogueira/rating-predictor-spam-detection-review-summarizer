# -*- coding: utf-8 -*-
"""13_dl_modelingVF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DZkez0tpO5phrvbUBgfYLe8SeHInq-Cz

## Setup & Imports
"""

import pandas as pd
import numpy as np
import joblib

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, CSVLogger

"""## Load Data & Features"""

df = pd.read_parquet("/content/engineered_features_with_anomalies.parquet")
structured_features = joblib.load("/content/feature_names_with_anomalies.json")

texts = df['clean_text'].astype(str).values
y = df['fake_review_label'].values

"""## Tokineze and Pad Text"""

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X_text = pad_sequences(sequences, maxlen=100)

joblib.dump(tokenizer, "/content/tokenizer.pkl")
np.save("/content/text_sequences.npy", X_text)

"""## Load Structure Feature and Combine Inputs"""

X_structured = df[structured_features].values
np.save("/content/X_structured.npy", X_structured)

from numpy import hstack
X_combined = hstack([X_text, X_structured])
np.save("/content/X_combined.npy", X_combined)

"""## Train / Test Split"""

X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y, test_size=0.2, stratify=y, random_state=42
)
np.save("/content/y_fake_review.npy", y)

"""## Build and Compile Model"""

model = Sequential([
    Embedding(input_dim=10000, output_dim=64),
    LSTM(64),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""## Train Model with Callbacks"""

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
csv_logger = CSVLogger("/content/training_log.csv")

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=10,
    batch_size=128,
    callbacks=[early_stop, csv_logger]
)

"""## Initial Evaluation (Threshold = 0.5)"""

from sklearn.metrics import classification_report

y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

"""## Plot Training Accuracy"""

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Training Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.tight_layout()
plt.show()

"""## Precision - Recall Curve and Threshoold Tuning"""

from sklearn.metrics import precision_recall_curve

y_proba = model.predict(X_test)
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Plot precision-recall vs threshold
import matplotlib.pyplot as plt
plt.plot(thresholds, precision[:-1], label='Precision')
plt.plot(thresholds, recall[:-1], label='Recall')
plt.xlabel("Threshold")
plt.legend()
plt.title("Precision vs Recall Tradeoff")
plt.show()

from numpy import argmax

f1_scores = 2 * (precision * recall) / (precision + recall)
best_idx = argmax(f1_scores)
best_threshold = thresholds[best_idx]
print(f"Optimal threshold for F1: {best_threshold:.2f}")

"""## Final Evaluation @ Optimal Threshold"""

y_pred_custom = (y_proba > best_threshold).astype(int)
print(classification_report(y_test, y_pred_custom))

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_custom)
plt.title("Confusion Matrix @ Optimal Threshold")
plt.show()

np.save("/content/y_pred_custom.npy", y_pred_custom)

"""## Save Threshold and Model Artifacts

"""

with open("/content/decision_threshold.txt", "w") as f:
    f.write(str(best_threshold))

model.save("/content/rnn_model.keras")
print("Model saved as rnn_model.keras")

log_df = pd.read_csv("/content/training_log.csv")
log_df.to_csv("/content/training_log_v1.csv", index=False)

"""## Predict on New Input

## New Section
"""

X_input = X_test
y_pred = (model.predict(X_input) > 0.23).astype(int)

import json

metadata = {
    "model_name": "rnn_model.keras",
    "threshold": float(best_threshold),
    "input_shape": list(X_train.shape[1:]),
    "features_used": structured_features,
    "tokenizer_path": "/content/tokenizer.pkl"
}

with open("/content/model_metadata.json", "w") as f:
    json.dump(metadata, f, indent=2)