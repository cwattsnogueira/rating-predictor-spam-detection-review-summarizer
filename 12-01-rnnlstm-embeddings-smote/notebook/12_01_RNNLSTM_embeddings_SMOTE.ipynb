{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rah_Ncz8S1aN"
      },
      "outputs": [],
      "source": [
        "# --- Install dependencies\n",
        "!pip install -q torch scikit-learn joblib imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
      ],
      "metadata": {
        "id": "AanDrvhGS7nD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A5Na3IkS9ix",
        "outputId": "56220944-d1aa-44d6-eb85-fe9faa7ddb23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (tune as needed)\n",
        "RNN_CONFIG = {\n",
        "    \"embedding_dim\": 100,   # if you use GloVe, match dim (100)\n",
        "    \"hidden_dim\": 128,\n",
        "    \"num_layers\": 1,\n",
        "    \"bidirectional\": True,\n",
        "    \"dropout\": 0.3,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 6,\n",
        "    \"lr\": 1e-3,\n",
        "    \"max_vocab_size\": 30000,\n",
        "    \"max_seq_len\": 200\n",
        "}"
      ],
      "metadata": {
        "id": "VAVeSB0gTAN9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/engineered_features.parquet\")\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "\n",
        "# Try to find label column automatically\n",
        "possible_label_cols = ['is_fake', 'fake', 'label', 'target', 'y']\n",
        "label_col = None\n",
        "for c in possible_label_cols:\n",
        "    if c in df.columns:\n",
        "        label_col = c\n",
        "        break\n",
        "\n",
        "if label_col is None:\n",
        "    # fallback: try to find any binary column with 0/1 values\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            vals = set(df[c].dropna().unique())\n",
        "            if vals.issubset({0,1}) or vals.issubset({0.0,1.0}):\n",
        "                label_col = c\n",
        "                break\n",
        "\n",
        "if label_col is None:\n",
        "    raise ValueError(\"No label column found. Please provide a binary label column (e.g. 'is_fake' with 0/1).\")\n",
        "\n",
        "print(\"Using label column:\", label_col)\n",
        "\n",
        "# Text column (same as earlier)\n",
        "text_col = \"clean_text\"\n",
        "if text_col not in df.columns:\n",
        "    # try other likely names\n",
        "    for c in ['text', 'review', 'reviews.text', 'review_text']:\n",
        "        if c in df.columns:\n",
        "            text_col = c\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(\"No text column found. Please ensure 'clean_text' or a similar column exists.\")\n",
        "\n",
        "print(\"Using text column:\", text_col)\n",
        "\n",
        "# Drop NaNs\n",
        "df = df[[text_col, label_col]].dropna().reset_index(drop=True)\n",
        "df[text_col] = df[text_col].astype(str)\n",
        "\n",
        "# Simple binary labels (0/1)\n",
        "y = df[label_col].astype(int).values\n",
        "texts = df[text_col].tolist()\n",
        "print(\"Examples:\", len(texts), \"labels distribution:\", np.bincount(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzjeTG-VTFOz",
        "outputId": "ed167758-a560-4088-8a87-3a3af7d5cbf5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Shape: (9480, 47)\n",
            "Using label column: purchase_missing_flag\n",
            "Using text column: clean_text\n",
            "Examples: 9480 labels distribution: [3447 6033]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train / Val / Test split + SMOTE oversampling\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "train_texts, test_texts, train_y, test_y = train_test_split(\n",
        "    texts, y, test_size=0.15, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "train_texts, val_texts, train_y, val_y = train_test_split(\n",
        "    train_texts, train_y, test_size=0.17647, random_state=RANDOM_SEED, stratify=train_y)\n",
        "# 70/15/15 split\n",
        "\n",
        "print(f\"Before SMOTE -> Train class distribution: {np.bincount(train_y)}\")\n",
        "\n",
        "# Convert text into numeric representation (TF-IDF) for SMOTE\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(train_texts)\n",
        "\n",
        "# Apply SMOTE to balance classes\n",
        "smote = SMOTE(random_state=RANDOM_SEED, sampling_strategy='auto')\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, train_y)\n",
        "\n",
        "# Recover balanced texts (approximate mapping)\n",
        "# We’ll rebuild the balanced text dataset using the same indices\n",
        "# Note: synthetic samples don't have direct text forms (SMOTE works in vector space),\n",
        "# so we’ll just duplicate some real examples to balance size for token-based models.\n",
        "\n",
        "majority_idx = np.where(train_y == np.bincount(train_y).argmax())[0]\n",
        "minority_idx = np.where(train_y == 1 - np.bincount(train_y).argmax())[0]\n",
        "\n",
        "# Compute how many samples were generated by SMOTE\n",
        "minority_upsampled_count = sum(y_resampled == (1 - np.bincount(train_y).argmax()))\n",
        "synthetic_needed = minority_upsampled_count - len(minority_idx)\n",
        "\n",
        "# Randomly duplicate minority texts to approximate the same balance for token-based models\n",
        "duplicated_texts = np.random.choice(np.array(train_texts)[minority_idx], size=synthetic_needed, replace=True)\n",
        "train_texts_balanced = list(train_texts) + duplicated_texts.tolist()\n",
        "train_y_balanced = np.concatenate([train_y, np.full(synthetic_needed, 1 - np.bincount(train_y).argmax())])\n",
        "\n",
        "print(f\"After SMOTE -> Balanced train size: {len(train_texts_balanced)} | class distribution: {np.bincount(train_y_balanced)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Sus6yFTTge",
        "outputId": "19d35da3-5d2a-4492-8547-b077c52e6959"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before SMOTE -> Train class distribution: [2413 4223]\n",
            "After SMOTE -> Balanced train size: 8446 | class distribution: [4223 4223]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization (simple whitespace split)\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return text.split()\n",
        "\n",
        "# Build vocabulary from training data\n",
        "from collections import Counter, defaultdict\n",
        "counter = Counter()\n",
        "for t in train_texts:\n",
        "    counter.update(tokenize(t))\n",
        "\n",
        "# Keep most common tokens\n",
        "max_vocab = RNN_CONFIG[\"max_vocab_size\"]\n",
        "most_common = counter.most_common(max_vocab - 2)  # reserve indices for PAD and UNK\n",
        "itos = ['<PAD>', '<UNK>'] + [w for w, _ in most_common]\n",
        "stoi = {w: i for i, w in enumerate(itos)}\n",
        "vocab_size = len(itos)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def text_to_sequence(tokens: List[str], stoi: Dict[str,int], max_len: int) -> List[int]:\n",
        "    seq = [stoi.get(t, stoi['<UNK>']) for t in tokens][:max_len]\n",
        "    if len(seq) < max_len:\n",
        "        seq = seq + [stoi['<PAD>']] * (max_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "# Option: initialize embedding matrix with preloaded GloVe (if available)\n",
        "glove_path = \"/content/glove.6B.100d.txt\"\n",
        "use_glove = os.path.exists(glove_path) and RNN_CONFIG[\"embedding_dim\"] == 100\n",
        "embedding_matrix = None\n",
        "\n",
        "if use_glove:\n",
        "    print(\"Loading GloVe for embedding initialization (this may take a minute)...\")\n",
        "    glove_map = {}\n",
        "    with open(glove_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            if len(parts) < 101:\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            vec = np.array(parts[1:], dtype='float32')\n",
        "            glove_map[word] = vec\n",
        "    # build embedding matrix\n",
        "    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, RNN_CONFIG[\"embedding_dim\"])).astype('float32')\n",
        "    for i, word in enumerate(itos):\n",
        "        if word in glove_map:\n",
        "            embedding_matrix[i] = glove_map[word]\n",
        "    print(\"Embedding matrix ready (with GloVe where available).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGj150CLTXmU",
        "outputId": "b27dc95c-af0d-4a9d-ce95-15e116add08f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 8403\n",
            "Loading GloVe for embedding initialization (this may take a minute)...\n",
            "Embedding matrix ready (with GloVe where available).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch dataset for RNN\n",
        "class RNNReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, stoi, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.stoi = stoi\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = tokenize(self.texts[idx])\n",
        "        seq = text_to_sequence(tokens, self.stoi, self.max_len)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(seq, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset_rnn = RNNReviewDataset(train_texts, train_y, stoi, RNN_CONFIG[\"max_seq_len\"])\n",
        "val_dataset_rnn   = RNNReviewDataset(val_texts, val_y, stoi, RNN_CONFIG[\"max_seq_len\"])\n",
        "test_dataset_rnn  = RNNReviewDataset(test_texts, test_y, stoi, RNN_CONFIG[\"max_seq_len\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset_rnn, batch_size=RNN_CONFIG[\"batch_size\"], shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset_rnn, batch_size=RNN_CONFIG[\"batch_size\"], shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset_rnn, batch_size=RNN_CONFIG[\"batch_size\"], shuffle=False)\n"
      ],
      "metadata": {
        "id": "kVInxl5GTkN8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM-based classifier\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=0.3, embedding_matrix=None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        if embedding_matrix is not None:\n",
        "            # freeze or not - we keep as trainable\n",
        "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), 2)  # binary: 2 classes\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        emb = self.embedding(input_ids)           # (batch, seq_len, emb_dim)\n",
        "        out, (hn, cn) = self.lstm(emb)            # out: (batch, seq_len, hidden*dir)\n",
        "        # use mean pooling over seq dimension\n",
        "        pooled = out.mean(dim=1)\n",
        "        x = self.dropout(pooled)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "f0AZwnpJTnNs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model\n",
        "model_rnn = LSTMClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    emb_dim=RNN_CONFIG[\"embedding_dim\"],\n",
        "    hidden_dim=RNN_CONFIG[\"hidden_dim\"],\n",
        "    num_layers=RNN_CONFIG[\"num_layers\"],\n",
        "    bidirectional=RNN_CONFIG[\"bidirectional\"],\n",
        "    dropout=RNN_CONFIG[\"dropout\"],\n",
        "    embedding_matrix=embedding_matrix\n",
        ").to(DEVICE)\n",
        "\n",
        "# Training utilities\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=RNN_CONFIG[\"lr\"])"
      ],
      "metadata": {
        "id": "-l3pw_HTTqhR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_rnn(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        lbls = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, lbls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * input_ids.size(0)\n",
        "        batch_preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
        "        preds.extend(batch_preds.tolist())\n",
        "        labels.extend(lbls.detach().cpu().numpy().tolist())\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "hb3cC2l6TsSM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_rnn(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            lbls = batch['labels'].to(device)\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, lbls)\n",
        "            total_loss += float(loss) * input_ids.size(0)\n",
        "            batch_preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            preds.extend(batch_preds.tolist())\n",
        "            labels.extend(lbls.cpu().numpy().tolist())\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    return avg_loss, acc, prec, rec, f1"
      ],
      "metadata": {
        "id": "6x6_eQUQTtZS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RNN\n",
        "best_val_f1 = 0.0\n",
        "print(\"\\n--- Training RNN (LSTM) ---\")\n",
        "for epoch in range(1, RNN_CONFIG[\"epochs\"] + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_epoch_rnn(model_rnn, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1 = eval_rnn(model_rnn, val_loader, criterion, DEVICE)\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch}/{RNN_CONFIG['epochs']} - train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {val_loss:.4f} val_acc {val_acc:.4f} val_f1 {val_f1:.4f} time {t1-t0:.1f}s\")\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model_rnn.state_dict(), \"/content/best_rnn_model.pt\")\n",
        "        print(\"  -> Saved best_rnn_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuMx7RVUTweq",
        "outputId": "2526a203-62ab-4ac2-cd5f-9ff25f921de7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training RNN (LSTM) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-956852357.py:14: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  total_loss += float(loss) * input_ids.size(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6 - train_loss 0.6252 train_acc 0.6644 | val_loss 0.5694 val_acc 0.7018 val_f1 0.7909 time 3.3s\n",
            "  -> Saved best_rnn_model.pt\n",
            "Epoch 2/6 - train_loss 0.5746 train_acc 0.7030 | val_loss 0.5599 val_acc 0.7138 val_f1 0.8021 time 1.8s\n",
            "  -> Saved best_rnn_model.pt\n",
            "Epoch 3/6 - train_loss 0.4713 train_acc 0.7669 | val_loss 0.4421 val_acc 0.7862 val_f1 0.8302 time 2.1s\n",
            "  -> Saved best_rnn_model.pt\n",
            "Epoch 4/6 - train_loss 0.3797 train_acc 0.8228 | val_loss 0.4191 val_acc 0.8066 val_f1 0.8533 time 1.9s\n",
            "  -> Saved best_rnn_model.pt\n",
            "Epoch 5/6 - train_loss 0.3592 train_acc 0.8373 | val_loss 0.4072 val_acc 0.8193 val_f1 0.8696 time 2.2s\n",
            "  -> Saved best_rnn_model.pt\n",
            "Epoch 6/6 - train_loss 0.3369 train_acc 0.8492 | val_loss 0.4292 val_acc 0.7975 val_f1 0.8435 time 1.9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "print(\"\\n--- RNN Test Evaluation ---\")\n",
        "model_rnn.load_state_dict(torch.load(\"/content/best_rnn_model.pt\", map_location=DEVICE))\n",
        "test_loss, test_acc, test_prec, test_rec, test_f1 = eval_rnn(model_rnn, test_loader, criterion, DEVICE)\n",
        "print(f\"RNN Test -> acc: {test_acc:.4f}, prec: {test_prec:.4f}, rec: {test_rec:.4f}, f1: {test_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyRASJpnTyNu",
        "outputId": "3e281af4-db7a-461d-c1b4-c335d39a7d04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RNN Test Evaluation ---\n",
            "RNN Test -> acc: 0.8094, prec: 0.7952, rec: 0.9436, f1: 0.8631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer & stoi/itos\n",
        "joblib.dump({'itos': itos, 'stoi': stoi}, \"/content/rnn_vocab.pkl\")\n",
        "print(\"RNN vocab saved to /content/rnn_vocab.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYa2vGBVT094",
        "outputId": "9788234a-a012-49fe-df41-341bdd3da4c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN vocab saved to /content/rnn_vocab.pkl\n"
          ]
        }
      ]
    }
  ]
}