{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning and Vectorization\n",
        "\n",
        "This notebook prepares the `reviews.text` field for NLP modeling. I clean the text, analyze review length, and vectorize it using CountVectorizer and TF-IDF. Each artifact is saved separately for downstream ML, DL, and fine-tuning workflows."
      ],
      "metadata": {
        "id": "57kIWRr8FHvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Cleaning and Vectorization\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "JQFaE02xGbAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloads\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGQwgjXPocUI",
        "outputId": "874ce8ee-a0e7-43d0-94c0-37ffc3a42073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "YtwxXaupoek7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/cleaned_reviews_with_categories.parquet\")\n",
        "print(\" Loaded:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IITVE2KGe1s",
        "outputId": "334c3f96-9027-4aa7-b0be-573a0b3d2ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded: (9480, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    # Remove digits\n",
        "    text = re.sub(r\"\\d+\", '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "    # Lemmatize\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Apply cleaning\n",
        "df['clean_text'] = df['reviews.text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "yTetBE6mGj9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review length analysis\n",
        "df['text_length'] = df['clean_text'].str.split().apply(len)\n",
        "df['is_short'] = df['text_length'] < 5\n",
        "print(f\"Short reviews (<5 words): {df['is_short'].sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpEMzURaGl2d",
        "outputId": "9b2e34dc-d6ab-4f4c-a437-18bae77e5d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short reviews (<5 words): 395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import joblib\n",
        "\n",
        "count_vec = CountVectorizer(max_features=5000)\n",
        "X_count = count_vec.fit_transform(df['clean_text'])\n",
        "joblib.dump(X_count, \"/content/X_count_vectorized.pkl\")\n",
        "joblib.dump(count_vec, \"/content/count_vectorizer.pkl\")\n",
        "print(\"Saved CountVectorizer and matrix.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TfA14sDGsiN",
        "outputId": "ef66bfc3-dbcc-421b-e25b-a6dd997a923e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved CountVectorizer and matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorizer\n",
        "tfidf_vec = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vec.fit_transform(df['clean_text'])\n",
        "joblib.dump(X_tfidf, \"/content/X_tfidf_vectorized.pkl\")\n",
        "joblib.dump(tfidf_vec, \"/content/tfidf_vectorizer.pkl\")\n",
        "print(\"Saved TF-IDF and matrix.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw3XpkSXGv68",
        "outputId": "98b473ae-1826-4982-fb1f-9b656d70c30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved TF-IDF and matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I cleaned and vectorized the review text. Each artifact was saved separately:\n",
        "\n",
        "- `cleaned_reviews_with_text.parquet`: full dataset with cleaned text\n",
        "- `X_count_vectorized.pkl`: CountVectorizer matrix\n",
        "- `count_vectorizer.pkl`: fitted CountVectorizer\n",
        "- `X_tfidf_vectorized.pkl`: TF-IDF matrix\n",
        "- `tfidf_vectorizer.pkl`: fitted TF-IDF\n",
        "\n",
        "These files are ready for ML, DL, and fine-tuning workflows."
      ],
      "metadata": {
        "id": "49Ii4pbvG7VT"
      }
    }
  ]
}