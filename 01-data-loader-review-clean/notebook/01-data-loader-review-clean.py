# -*- coding: utf-8 -*-
"""01_data_loader_review_cleanVF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eMZ1ECQQMAqmexCi_RlZXVgQFaIhND9h

# Data Loader & Initial Cleaning

This is the first notebook in my final AI/ML bootcamp project. I begin by loading the raw review dataset and inspecting its structure. I check for missing values, duplicated reviews, and incorrect data types.

My goal is to clean the dataset while preserving the most informative features for modeling. I document each decision and save a cleaned version for downstream use.
"""

# Import core libraries
import pandas as pd
import numpy as np

# Load raw dataset
file_path = "/content/raw_reviews.csv"
df = pd.read_csv(file_path)

# Preview the data
print(" Initial data preview:")
display(df.head())

print("\n Raw Data shape:", df.shape)

# Check for missing values
print("\n Raw Missing values per column:")
missing_summary = df.isnull().sum().sort_values(ascending=False)
display(missing_summary)

print("\n Raw Column types:")
display(df.dtypes)

df["reviews.numHelpful"].unique()

df["reviews.doRecommend"].unique()

df["reviews.didPurchase"].unique()

df["ean"].unique()

df["manufacturerNumber"].unique()

# Drop columns with excessive missingness or low modeling value
drop_cols = [
    'reviews.userCity', 'reviews.userProvince', 'ean', 'reviews.id',
    ]
df_cleaned = df.drop(columns=drop_cols, errors='ignore')

# reviews.userProvince -->	9964 NULLS
# reviews.userCity -->	8703 NULLS
# reviews.id	--> 6364 NULLS
# ean	3135 --> NULLS

# Check for missing values
print("\n df_cleaned Missing values per column part1:")
print(df_cleaned.isnull().sum())

# Drop rows missing critical fields
df_cleaned = df_cleaned.dropna(subset=['reviews.text'])
# reviews.text --> 2 NULLS

df_cleaned = df_cleaned.dropna(subset=['reviews.date'])
# reviews.date --> 14 NULLS

df_cleaned = df_cleaned.dropna(subset=['reviews.title'])
# reviews.title --> 56 NULLS

df_cleaned = df_cleaned.dropna(subset=['reviews.username'])
# reviews.username --> 45 NULLS

df_cleaned = df_cleaned.dropna(subset=['manufacturerNumber'])
# manufacturerNumber --> 18 NULLS

# Check for missing values
print("\n df_cleaned Missing values per column part2:")
print(df_cleaned.isnull().sum())

# Reset index
df_cleaned.reset_index(drop=True, inplace=True)

def extract_first_date(date_str):
    if pd.isna(date_str):
        return np.nan
    if isinstance(date_str, pd.Timestamp):
        return date_str
    if isinstance(date_str, str):
        return date_str.split(',')[0]
    return np.nan

df_cleaned['reviews.dateSeen'] = df_cleaned['reviews.dateSeen'].apply(extract_first_date)

df_cleaned['reviews.dateSeen'] = pd.to_datetime(df_cleaned['reviews.dateSeen'], errors='coerce', utc=True)

print("\n Final dtype of reviews.dateSeen:", df_cleaned['reviews.dateSeen'].dtype)
print(" Missing values in reviews.dateSeen:", df_cleaned['reviews.dateSeen'].isna().sum())

# Convert date columns to datetime safely
date_cols = ['dateAdded', 'dateUpdated', 'reviews.dateAdded', 'reviews.date']

for col in date_cols:
    if col in df_cleaned.columns:
        # Preview sample values
        print(f"\n Sample values from {col}:")
        print(df_cleaned[col].dropna().head(3).tolist())

        # Apply flexible conversion
        df_cleaned[col] = pd.to_datetime(df_cleaned[col], errors='coerce', utc=True)

# Confirm conversion
print("\n Final column types after datetime conversion:")
print(df_cleaned[date_cols].dtypes)

print("\n Final Column types:")
print(df_cleaned.dtypes)

"""## Duplicated"""

# Duplicated review text analysis
print(f"\n Duplicated review texts: {df.duplicated(subset='reviews.text').sum()}")
# Duplicated review texts: 211

# Remove duplicated review texts (keep first occurrence)
df_cleaned = df_cleaned.drop_duplicates(subset='reviews.text', keep='first').reset_index(drop=True)

print(f" Remaining reviews after dropping duplicates: {df_cleaned.shape[0]}")
# I removed 211 duplicated review entries based on identical `reviews.text`.

# same text, same title
duplicate_text_title = df_cleaned[df_cleaned.duplicated(subset=['reviews.text', 'reviews.title'], keep=False)]
print(f"\nFound {duplicate_text_title.shape[0]} duplicated entries based on 'reviews.text' and 'reviews.title'.")
display(duplicate_text_title[['reviews.text', 'reviews.title', 'reviews.username']].head(10))
# Found 0 duplicated entries based on 'reviews.text' and 'reviews.title'.

# Same user, same data, same title
duplicate_user_date_title = df_cleaned[df_cleaned.duplicated(subset=['reviews.username', 'reviews.date', 'reviews.title'], keep=False)]
print(f"\nFound {duplicate_user_date_title.shape[0]} duplicated entries based on 'username', 'date', and 'title'.")
display(duplicate_user_date_title[['reviews.username', 'reviews.date', 'reviews.title', 'reviews.text']].head(10))
# Found 34 duplicated entries based on 'username', 'date', and 'title'.

# Check for duplicated review texts
duplicate_reviews_b = df_cleaned[df_cleaned.duplicated(subset='reviews.text', keep=False)]
print(f"\n Found {duplicate_reviews_b.shape[0]} duplicated review entries based on 'reviews.text'.")
display(duplicate_reviews_b[['reviews.text', 'reviews.username']].head(10))
# Found 0 duplicated review entries based on 'reviews.text'.

duplicate_product_manufacturer = df_cleaned[df_cleaned.duplicated(subset=['upc', 'manufacturer'], keep=False)]
print(f"\nFound {duplicate_product_manufacturer.shape[0]} duplicated entries based on 'upc' and 'manufacturer'.")
display(duplicate_product_manufacturer[['upc', 'manufacturer', 'name']].head(10))
# Found 9463 duplicated entries based on 'upc' and 'manufacturer'.

# Check for duplicated review texts
duplicate_reviews_z = df_cleaned[df_cleaned.duplicated(subset='upc', keep=False)]
print(f"\n Found {duplicate_reviews_z.shape[0]} duplicated review entries based on 'upc'.")
display(duplicate_reviews_z[['upc', 'reviews.username', 'reviews.date']].head(10))
# Found 9466 duplicated review entries based on 'upc'.

duplicate_source_text = df_cleaned[df_cleaned.duplicated(subset=['reviews.sourceURLs', 'reviews.text'], keep=False)]
print(f"\nFound {duplicate_source_text.shape[0]} duplicated entries based on 'sourceURLs' and 'reviews.text'.")
display(duplicate_source_text[['reviews.sourceURLs', 'reviews.text', 'reviews.username']].head(10))
# Found 0 duplicated entries based on 'sourceURLs' and 'reviews.text'.

duplicate_manu_code = df_cleaned[df_cleaned.duplicated(subset=['manufacturerNumber', 'name'], keep=False)]
print(f"\nFound {duplicate_manu_code.shape[0]} duplicated entries based on 'manufacturerNumber' and 'name'.")
display(duplicate_manu_code[['manufacturerNumber', 'name', 'upc']].head(10))
# Found 9462 duplicated entries based on 'manufacturerNumber' and 'name'.

# Check for duplicated review texts
duplicate_reviews_a = df_cleaned[df_cleaned.duplicated(subset=['upc', 'reviews.username', 'reviews.date'], keep=False)]
print(f"\n Found {duplicate_reviews_a.shape[0]} duplicated review entries based on 'upc', 'reviews.username', 'reviews.date'.")
display(duplicate_reviews_a[['upc', 'name', 'reviews.text', 'reviews.username', 'reviews.date']].head(10))
# Found 84 duplicated review entries based on 'upc', 'reviews.username', 'reviews.date'.

# Check for duplicated review texts
duplicate_reviews_u = df_cleaned[df_cleaned.duplicated(subset=['upc', 'name', 'reviews.text', 'reviews.username'], keep=False)]
print(f"\n Found {duplicate_reviews_u.shape[0]} duplicated review entries based on 'upc', 'name', 'reviews.text', 'reviews.username'.")
display(duplicate_reviews_u[['upc', 'name', 'reviews.text', 'reviews.username']].head(10))
# Found 0 duplicated review entries based on 'upc', 'name', 'reviews.text', 'reviews.username'

# Check for duplicated UPCs
duplicate_upc_u = df_cleaned[df_cleaned.duplicated(subset=['upc', 'reviews.username'], keep=False)]
print(f"\nFound {duplicate_upc_u.shape[0]} duplicated entries based on 'upc' and 'reviews.username'.")
display(duplicate_upc_u[['upc', 'name', 'reviews.text', 'reviews.username']].head(10))
# Found 311 duplicated entries based on 'upc'.

# Check for duplicated UPC + review text
duplicate_upc_text = df_cleaned[df_cleaned.duplicated(subset=['upc', 'reviews.text'], keep=False)]
print(f"\nFound {duplicate_upc_text.shape[0]} duplicated entries based on 'upc' and 'reviews.text'.")
display(duplicate_upc_text[['upc', 'name', 'reviews.text', 'reviews.username']].head(10))
# Found 0 duplicated entries based on 'upc' and 'reviews.text'.

# check on df_cleaned["reviews.username"] if there is same username repeat

# Count how many times each username appears
username_counts = df['reviews.username'].value_counts()

# Filter only usernames that appear more than once
duplicate_usernames = username_counts[username_counts > 1]

# Display result
print(f" Found {len(duplicate_usernames)} usernames with multiple reviews.")
display(duplicate_usernames.head(10))  # Show top 10
# Found 358 usernames with multiple reviews.

"""## Flag"""

# Step 1: Flag missing
df_cleaned['purchase_missing_flag'] = df_cleaned['reviews.didPurchase'].isna().astype(int)

# Step 2: Map to status
def purchase_status(x):
    if pd.isna(x):
        return 'unknown'
    elif x is True:
        return 'purchased'
    else:
        return 'not_purchased'

df_cleaned['purchase_status'] = df_cleaned['reviews.didPurchase'].apply(purchase_status)

# Step 3: Encode as ordinal
purchase_map = {'unknown': 0, 'not_purchased': 1, 'purchased': 2}
df_cleaned['purchase_encoded'] = df_cleaned['purchase_status'].map(purchase_map)

# Step 1: Flag missing
df_cleaned['recommend_missing_flag'] = df_cleaned['reviews.doRecommend'].isna().astype(int)

# Step 2: Map to status
def recommend_status(x):
    if pd.isna(x):
        return 'unknown'
    elif x is True:
        return 'recommended'
    else:
        return 'not_recommended'

df_cleaned['recommend_status'] = df_cleaned['reviews.doRecommend'].apply(recommend_status)

# Step 3: Encode as ordinal
recommend_map = {'unknown': 0, 'not_recommended': 1, 'recommended': 2}
df_cleaned['recommend_encoded'] = df_cleaned['recommend_status'].map(recommend_map)

df_cleaned.drop(columns=['reviews.didPurchase', 'reviews.doRecommend'], inplace=True)

# Step 1: Create missing flag BEFORE filling
df_cleaned['helpful_missing_flag'] = df_cleaned['reviews.numHelpful'].isna().astype(int)

# Step 2: Fill missing with 0 (assume no votes yet)
df_cleaned['reviews.numHelpful'] = df_cleaned['reviews.numHelpful'].fillna(0)

# Step 3: Create binary flag and log-normalized version
df_cleaned['no_helpful_votes_flag'] = (df_cleaned['reviews.numHelpful'] == 0).astype(int)
df_cleaned['log_helpful'] = np.log1p(df_cleaned['reviews.numHelpful'])

df_cleaned = df_cleaned.dropna(subset=['reviews.date', 'reviews.dateSeen'])

# Missing values (still present)
print("\n Check Final Remaining missing values:")
print(df_cleaned.isnull().sum())

# After deduplication and safe datetime conversion, only 146 entries in `reviews.dateSeen` remain missing — just 1.49% of the dataset.
# I chose to retain this column, as it may provide useful temporal signals or be transformed into a binary feature (`has_dateSeen`).

# After reviewing missing values in reviews.title and reviews.username, I chose to fill them with "notitle" and "anonymous" respectively.
# These substitutions preserve dataset integrity, prevent null-related issues during modeling, and clearly signal the absence of user-provided content.

# Save cleaned dataset
output_path = "/content/cleaned_reviews.parquet"
df_cleaned.to_parquet(output_path, index=False)

print("\n Cleaned dataset saved to:", output_path)
print(" Final shape:", df_cleaned.shape)

"""## Unique values"""

df_cleaned["reviews.sourceURLs"].unique()

df_cleaned["brand"].unique()

df_cleaned["keys"].unique()

df_cleaned["manufacturer"].unique()

df_cleaned["name"].unique()

df_cleaned["reviews.title"].unique()

df_cleaned["reviews.rating"].unique()

df_cleaned["reviews.username"].unique()

df_cleaned["upc"].unique()

# view only df_cleaned["reviews.text"]["name"].head()
df_cleaned[["reviews.text", "name"]].head()

"""## Report

##  Updated Column Review and Cleaning Decisions

##  Final Column Review and Cleaning Decisions

| Column Name             | Description                                      | Missing % | Decision               | Reasoning |
|-------------------------|--------------------------------------------------|-----------|------------------------|-----------|
| `id`                    | Product ID                                       | 0%        | Keep                   | Unique identifier |
| `brand`                 | Brand name                                       | 0%        | Keep                   | Useful for grouping |
| `categories`            | Product categories                               | 0%        | Keep                   | May help in analysis |
| `dateAdded`             | Date product was added                           | 0%        | Keep                   | Converted to datetime |
| `dateUpdated`           | Last update date                                 | 0%        | Keep                   | Converted to datetime |
| `ean`                   | European Article Number                          | 31.3%     | Drop                   | High missing rate, redundant |
| `keys`                  | Internal product keys                            | 0%        | Keep                   | May help in traceability |
| `manufacturer`          | Manufacturer name                                | 0%        | Keep                   | Useful for grouping |
| `manufacturerNumber`    | Manufacturer product code                        | 0.18%     | Drop                   | Low signal |
| `name`                  | Product name                                     | 0%        | Keep                   | Core identifier |
| `reviews.date`          | Review date                                      | 0.14%     | Keep + drop rows       | Converted to datetime; rows with missing dates removed |
| `reviews.dateAdded`     | Review creation date                             | 0%        | Keep                   | Converted to datetime |
| `reviews.dateSeen`      | Date review was seen                             | 1.49%     | Keep + drop rows       | Converted to datetime; rows with missing tracking removed |
| `reviews.didPurchase`   | Purchase confirmation                            | 63.6%     | Keep + encode + flag   | Encoded as ordinal with `"unknown"` category; preserves behavioral signal |
| `reviews.doRecommend`   | User recommendation                              | 5.6%      | Keep + encode + flag   | Encoded as ordinal with `"unknown"` category; captures sentiment and silence |
| `reviews.id`            | Review ID                                        | 63.6%     | Drop                   | Sparse and redundant |
| `reviews.numHelpful`    | Helpful votes                                    | 57.5%     | Keep + fill + log + flag | Filled with 0, log-normalized, and flagged for missingness and zero engagement |
| `reviews.rating`        | Star rating (1–5)                                | 0%        | Keep                   | Core feature |
| `reviews.sourceURLs`    | Source URL of the review                         | 0%        | Keep                   | May help in traceability |
| `reviews.text`          | Review content                                   | 0.02%     | Keep + drop rows       | Core feature for NLP; rows with missing text removed |
| `reviews.title`         | Review title                                     | 0.56%     | Keep + drop rows       | Important for summarization; rows with missing titles removed |
| `reviews.userCity`      | User city                                        | 87%       | Drop                   | Very sparse |
| `reviews.userProvince`  | User province/state                              | 99.6%     | Drop                   | Extremely sparse |
| `reviews.username`      | Username of reviewer                             | 0.45%     | Keep + drop rows       | Useful for deduplication and user-level analysis; rows with missing usernames removed |
| `upc`                   | Universal Product Code                           | 0%        | Keep                   | Useful for product matching |
"""