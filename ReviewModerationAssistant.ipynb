{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwx31PqL7van0EenesFaMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwattsnogueira/rating-predictor-spam-detection-review-summarizer/blob/main/ReviewModerationAssistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upload your ZIP file (bert_oversampling_model.zip)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Extract filename\n",
        "import os\n",
        "zip_filename = next(iter(uploaded))  # gets the first uploaded file\n",
        "\n",
        "# Step 3: Create target folder\n",
        "target_folder = \"/content/\"\n",
        "os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "# Step 4: Unzip into the folder\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_folder)\n",
        "\n",
        "print(f\" Unzipped '{zip_filename}' into '{target_folder}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "P3YsXh53eSng",
        "outputId": "04281f11-7393-4f48-e26f-e37171dd8f04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e80ac9ff-4858-4bfe-b02b-6c92a6299b04\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e80ac9ff-4858-4bfe-b02b-6c92a6299b04\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bert_oversampling_model.zip to bert_oversampling_model.zip\n",
            " Unzipped 'bert_oversampling_model.zip' into '/content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 1: install dependencies (run once)\n",
        "!pip install -q transformers gradio torch pandas scikit-learn joblib textblob tqdm\n",
        "# optional: if TextBlob needs corpora it may warn; for safety we will fall back gracefully if missing."
      ],
      "metadata": {
        "id": "oK5XuuVzIeO9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell 2: main app\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import gradio as gr\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "E6Al6QTHIkbX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------- CONFIG -------------\n",
        "MODEL_DIR = \"/content/bert_oversampling_model\"  # update if needed\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "u3aXNcXHImlb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------- HELPERS -------------\n",
        "def safe_read_csv(uploaded_file):\n",
        "    \"\"\"\n",
        "    Robust CSV reader that handles Gradio uploads, paths, or file-like objects.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if hasattr(uploaded_file, \"name\"):\n",
        "            return pd.read_csv(uploaded_file.name, encoding='utf-8', engine='python', on_bad_lines='skip')\n",
        "        if isinstance(uploaded_file, str) and os.path.exists(uploaded_file):\n",
        "            return pd.read_csv(uploaded_file, encoding='utf-8', engine='python', on_bad_lines='skip')\n",
        "        if hasattr(uploaded_file, \"read\"):\n",
        "            content = uploaded_file.read()\n",
        "            if isinstance(content, bytes):\n",
        "                try:\n",
        "                    s = content.decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    s = content.decode('latin1')\n",
        "            else:\n",
        "                s = content\n",
        "            return pd.read_csv(io.StringIO(s), engine='python', on_bad_lines='skip')\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to read CSV: {e}\")\n",
        "    raise RuntimeError(\"Unsupported file type for CSV input.\")"
      ],
      "metadata": {
        "id": "Gb12-4kRIqJ8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column name mapping heuristics\n",
        "PRODUCT_CANDIDATES = [\"product_name\", \"product\", \"name\", \"productName\"]\n",
        "TITLE_CANDIDATES = [\"review_title\", \"title\", \"reviewTitle\"]\n",
        "TEXT_CANDIDATES = [\"review_text\", \"review\", \"text\", \"reviews.text\", \"review_text_clean\", \"clean_text\"]\n",
        "\n",
        "def map_columns(df):\n",
        "    cols = df.columns.str.lower()\n",
        "    col_map = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    def choose(candidates):\n",
        "        for c in candidates:\n",
        "            if c in col_map:\n",
        "                return col_map[c]\n",
        "        return None\n",
        "\n",
        "    product_col = choose(PRODUCT_CANDIDATES)\n",
        "    title_col = choose(TITLE_CANDIDATES)\n",
        "    text_col = choose(TEXT_CANDIDATES)\n",
        "\n",
        "    if text_col is None:\n",
        "        lengths = {}\n",
        "        for c in df.columns:\n",
        "            try:\n",
        "                sample = df[c].dropna().astype(str).head(100)\n",
        "                avg_len = sample.str.split().apply(len).mean()\n",
        "                lengths[c] = avg_len\n",
        "            except Exception:\n",
        "                lengths[c] = 0\n",
        "        candidate = max(lengths, key=lambda k: lengths[k])\n",
        "        if lengths[candidate] > 3:\n",
        "            text_col = candidate\n",
        "\n",
        "    return product_col, title_col, text_col"
      ],
      "metadata": {
        "id": "edTAqhAqIq5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heuristics for explainability\n",
        "URL_RE = re.compile(r'https?://\\S+')\n",
        "EMAIL_RE = re.compile(r'\\S+@\\S+')\n",
        "PHONE_RE = re.compile(r'(\\+?\\d[\\d\\-\\s]{6,}\\d)')"
      ],
      "metadata": {
        "id": "FNgpTNBzIuYl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_KEYWORDS = {\n",
        "    'skincare': ['cream','moisturizer','soap','lotion','cleanser'],\n",
        "    'haircare': ['shampoo','conditioner','hair'],\n",
        "    'makeup': ['lipstick','foundation','concealer','makeup'],\n",
        "    'fragrance': ['perfume','scent','aroma','fragrance'],\n",
        "    'food': ['taste','sauce','cake','pizza','ketchup','flavor'],\n",
        "    'household': ['detergent','cleaner','glass','spray'],\n",
        "    'home': ['lamp','bed','furniture','shelf','blanket'],\n",
        "    'entertainment': ['movie','dvd','series','album']\n",
        "}"
      ],
      "metadata": {
        "id": "dRF0V9fOIwgv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_heuristics(product, title, review_text):\n",
        "    text = (\"\" if pd.isna(review_text) else str(review_text)).strip()\n",
        "    product = (\"\" if pd.isna(product) else str(product)).strip().lower()\n",
        "\n",
        "    tokens = re.findall(r'\\w+', text.lower())\n",
        "    token_count = len(tokens)\n",
        "\n",
        "    repetition_score = 0.0\n",
        "    if token_count:\n",
        "        vc = pd.Series(tokens).value_counts(normalize=True)\n",
        "        repetition_score = float(vc.max())\n",
        "\n",
        "    product_overlap = 0\n",
        "    if product:\n",
        "        pname_tokens = set(re.findall(r'\\w+', product))\n",
        "        product_overlap = int(len(pname_tokens & set(tokens)) > 0)\n",
        "\n",
        "    has_url = int(bool(URL_RE.search(text)))\n",
        "    has_email = int(bool(EMAIL_RE.search(text)))\n",
        "    has_phone = int(bool(PHONE_RE.search(text)))\n",
        "\n",
        "    try:\n",
        "        polarity = TextBlob(text).sentiment.polarity\n",
        "    except Exception:\n",
        "        polarity = 0.0\n",
        "\n",
        "    short_extreme_flag = int((token_count < 6) and (abs(polarity) > 0.8))\n",
        "\n",
        "    unrelated_flag = 0\n",
        "    for cat, keywords in CATEGORY_KEYWORDS.items():\n",
        "        for kw in keywords:\n",
        "            if re.search(r'\\b' + re.escape(kw) + r'\\b', text.lower()):\n",
        "                if product and kw not in product:\n",
        "                    unrelated_flag = 1\n",
        "                elif not product:\n",
        "                    unrelated_flag = 1\n",
        "\n",
        "    return {\n",
        "        \"token_count\": int(token_count),\n",
        "        \"repetition_score\": repetition_score,\n",
        "        \"product_overlap\": int(product_overlap),\n",
        "        \"has_url\": has_url,\n",
        "        \"has_email\": has_email,\n",
        "        \"has_phone\": has_phone,\n",
        "        \"polarity\": float(polarity),\n",
        "        \"short_extreme_flag\": short_extreme_flag,\n",
        "        \"unrelated_flag\": unrelated_flag\n",
        "    }"
      ],
      "metadata": {
        "id": "Gkb0PTsNIz10"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------- MODEL LOADING --------------\n",
        "print(\"Loading model...\")\n",
        "if not os.path.isdir(MODEL_DIR):\n",
        "    raise RuntimeError(f\"Model directory {MODEL_DIR} not found.\")\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Model loaded on\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Rr-ysII13g",
        "outputId": "1d2cb9c6-8526-4872-c645-3a5813566ba4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------- PREDICTION --------------\n",
        "def batch_predict_texts(texts, batch_size=BATCH_SIZE):\n",
        "    preds, probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = tokenizer(batch_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "            inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            batch_probs = softmax(logits, dim=-1).cpu().numpy()\n",
        "            batch_preds = np.argmax(batch_probs, axis=1)\n",
        "            preds.extend(batch_preds.tolist())\n",
        "            probs.extend(batch_probs.tolist())\n",
        "    return preds, probs"
      ],
      "metadata": {
        "id": "PIvmo6jfI4Mc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- CLASSIFICATION FUNCTIONS ----------------\n",
        "def classify_review(product, title, review, reviewer=None, date=None):\n",
        "    import re\n",
        "    from textblob import TextBlob\n",
        "\n",
        "    # Combine title and review\n",
        "    text = f\"{title.strip()}. {review.strip()}\" if title else review.strip()\n",
        "\n",
        "    # Heuristics\n",
        "    tokens = text.split()\n",
        "    token_count = len(tokens)\n",
        "    repetition_score = max([tokens.count(t) for t in set(tokens)]) / token_count if token_count > 0 else 0\n",
        "    product_overlap = 1 if product.lower() in text.lower() else 0\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    short_extreme_flag = token_count < 12 and polarity > 0.7\n",
        "    unrelated_flag = any(kw in text.lower() for kw in [\"buy now\", \"click here\", \"visit site\"])\n",
        "    has_url = bool(re.search(r\"http[s]?://\", text))\n",
        "    has_email = bool(re.search(r\"\\S+@\\S+\", text))\n",
        "    has_phone = bool(re.search(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\", text))\n",
        "\n",
        "    heur = {\n",
        "        \"token_count\": token_count,\n",
        "        \"repetition_score\": repetition_score,\n",
        "        \"product_overlap\": product_overlap,\n",
        "        \"polarity\": polarity,\n",
        "        \"short_extreme_flag\": short_extreme_flag,\n",
        "        \"unrelated_flag\": unrelated_flag,\n",
        "        \"has_url\": has_url,\n",
        "        \"has_email\": has_email,\n",
        "        \"has_phone\": has_phone\n",
        "    }\n",
        "\n",
        "    # Model prediction\n",
        "    preds, probs = batch_predict_texts([text])\n",
        "    prob = probs[0]\n",
        "    fake_score = float(prob[1])\n",
        "    genuine_score = float(prob[0])\n",
        "\n",
        "    # Heuristic post-processing (recalibrated)\n",
        "    if polarity < -0.15 and \"refund\" in text.lower():\n",
        "        fake_score -= 0.2  # rescue negative reviews with refund\n",
        "    if token_count < 12 and polarity > 0.5:\n",
        "        fake_score += 0.15  # short + strongly positive → suspicious\n",
        "    if product_overlap and repetition_score > 0.2:\n",
        "        fake_score += 0.2  # repeated product name + promo\n",
        "    if token_count > 25 and polarity < 0.3 and repetition_score < 0.1:\n",
        "        fake_score -= 0.1  # long, neutral, low repetition → likely genuine\n",
        "\n",
        "    # Normalize\n",
        "    fake_score = max(0, min(1, fake_score))\n",
        "    genuine_score = 1 - fake_score\n",
        "    pred = 1 if fake_score > 0.6 else 0\n",
        "\n",
        "    label = \"⚠️ Fake\" if pred == 1 else \"✅ Genuine\"\n",
        "    suggestion = \"This review may contain vague or promotional language. Manual check advised.\" if pred == 1 else \"This review appears authentic with specific details.\"\n",
        "\n",
        "    explanation_lines = [\n",
        "        f\"- Token count: {heur['token_count']}\",\n",
        "        f\"- Repetition score: {heur['repetition_score']:.2f}\",\n",
        "        f\"- Product name overlap: {heur['product_overlap']}\",\n",
        "        f\"- Sentiment polarity (TextBlob): {heur['polarity']:.2f}\",\n",
        "        f\"- Short & extreme sentiment: {bool(heur['short_extreme_flag'])}\",\n",
        "        f\"- Unrelated keyword flag: {heur['unrelated_flag']}\"\n",
        "    ]\n",
        "    if has_url or has_email or has_phone:\n",
        "        explanation_lines.append(f\"- Contains URL/email/phone: url={has_url}, email={has_email}, phone={has_phone}\")\n",
        "\n",
        "    explanation = \"\\n\".join(explanation_lines)\n",
        "\n",
        "    out_md = f\"\"\"**Prediction:** {label}\n",
        "**Confidence:** {fake_score*100:.2f}% if Fake, {genuine_score*100:.2f}% if Genuine\n",
        "\n",
        "**Class 0 (Genuine):** {genuine_score:.2%}\n",
        "**Class 1 (Fake):** {fake_score:.2%}\n",
        "\n",
        "**Suggestion:** {suggestion}\n",
        "\n",
        "**Heuristics / Explanation:**\n",
        "{explanation}\n",
        "\"\"\"\n",
        "    return out_md"
      ],
      "metadata": {
        "id": "aQd5x4sGIhCe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_classify(uploaded_file):\n",
        "    try:\n",
        "        df = safe_read_csv(uploaded_file)\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error reading CSV: {e}\"\n",
        "\n",
        "    if df.shape[0] == 0:\n",
        "        return \"❌ CSV is empty.\"\n",
        "\n",
        "    product_col, title_col, text_col = map_columns(df)\n",
        "    if text_col is None:\n",
        "        return \"❌ Could not find a review text column.\"\n",
        "\n",
        "    df['__product'] = df[product_col] if product_col in df.columns else \"\"\n",
        "    df['__title'] = df[title_col] if title_col in df.columns else \"\"\n",
        "    df['__text'] = df[text_col]\n",
        "\n",
        "    texts = (df['__title'].fillna('').astype(str).str.strip() + \". \" + df['__text'].fillna('').astype(str)).tolist()\n",
        "    preds, probs = batch_predict_texts(texts)\n",
        "\n",
        "    results = []\n",
        "    for i, row in df.reset_index(drop=True).iterrows():\n",
        "        text = f\"{row['__title']}. {row['__text']}\"\n",
        "        prob = probs[i]\n",
        "        fake_score = float(prob[1])\n",
        "        genuine_score = float(prob[0])\n",
        "\n",
        "        # Heuristics\n",
        "        tokens = text.split()\n",
        "        token_count = len(tokens)\n",
        "        repetition_score = max([tokens.count(t) for t in set(tokens)]) / token_count if token_count > 0 else 0\n",
        "        product_overlap = 1 if row['__product'].lower() in text.lower() else 0\n",
        "        polarity = TextBlob(text).sentiment.polarity\n",
        "\n",
        "        # Heuristic post-processing (recalibrated)\n",
        "        if polarity < -0.15 and \"refund\" in text.lower():\n",
        "            fake_score -= 0.2\n",
        "        if token_count < 12 and polarity > 0.5:\n",
        "            fake_score += 0.15\n",
        "        if product_overlap and repetition_score > 0.2:\n",
        "            fake_score += 0.2\n",
        "        if token_count > 25 and polarity < 0.3 and repetition_score < 0.1:\n",
        "            fake_score -= 0.1\n",
        "\n",
        "        fake_score = max(0, min(1, fake_score))\n",
        "        genuine_score = 1 - fake_score\n",
        "        pred = 1 if fake_score > 0.6 else 0\n",
        "\n",
        "        label = \"Fake\" if pred == 1 else \"Genuine\"\n",
        "        suggestion = \"May contain vague or promotional language.\" if pred == 1 else \"Likely authentic.\"\n",
        "        explanation = f\"tokens={token_count}, repetition={repetition_score:.2f}, product_overlap={product_overlap}, polarity={polarity:.2f}\"\n",
        "\n",
        "        results.append({\n",
        "            \"product_name\": row['__product'],\n",
        "            \"review_title\": row['__title'],\n",
        "            \"review_text\": row['__text'],\n",
        "            \"prediction\": label,\n",
        "            \"confidence\": f\"{fake_score*100:.2f}%\" if pred == 1 else f\"{genuine_score*100:.2f}%\",\n",
        "            \"class_0_genuine\": f\"{genuine_score:.2%}\",\n",
        "            \"class_1_fake\": f\"{fake_score:.2%}\",\n",
        "            \"suggestion\": suggestion,\n",
        "            \"explanation\": explanation\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "hzv_wXgabDYH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------- GRADIO UI --------------\n",
        "with gr.Blocks(title=\"Review Moderation Assistant\") as demo:\n",
        "    gr.Markdown(\"##  Review Moderation Assistant\\nUpload a CSV or evaluate a single review. The model returns a prediction, confidence, and rule-based explanation flags.\")\n",
        "    with gr.Tab(\"Single Review\"):\n",
        "        product = gr.Textbox(label=\"Product Name\", placeholder=\"e.g. Acme Face Cream\")\n",
        "        title = gr.Textbox(label=\"Review Title\", placeholder=\"Optional short title\")\n",
        "        review = gr.Textbox(label=\"Review Text\", lines=6, placeholder=\"Paste review text here\")\n",
        "        reviewer = gr.Textbox(label=\"Reviewer (optional)\")\n",
        "        date = gr.Textbox(label=\"Review Date (optional)\")\n",
        "        eval_btn = gr.Button(\"Evaluate Review\")\n",
        "        out_md = gr.Markdown()\n",
        "        eval_btn.click(fn=classify_review, inputs=[product, title, review, reviewer, date], outputs=out_md)\n",
        "\n",
        "    with gr.Tab(\"Batch Review (CSV)\"):\n",
        "        gr.Markdown(\"CSV must contain a review text column. The app will try to map common column names: `review_text`, `review`, `text`.\")\n",
        "        file_input = gr.File(label=\"Upload CSV\", file_types=[\".csv\", \".txt\"])\n",
        "        run_btn = gr.Button(\"Run CSV Classification\")\n",
        "        batch_output = gr.DataFrame()\n",
        "        run_btn.click(fn=batch_classify, inputs=file_input, outputs=batch_output)\n",
        "\n",
        "    gr.Markdown(\"### Notes\\n- Place your trained model in `/content/bert_oversampling_model`.\\n- Example CSV columns: `product_name`, `review_title`, `review_text`\")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "7xMsra4-JAE7",
        "outputId": "f0e3a66e-b698-4a0a-b01d-b682251c967d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://885978abb8416d6513.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://885978abb8416d6513.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}